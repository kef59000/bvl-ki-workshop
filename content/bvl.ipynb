{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4022a1d6",
   "metadata": {},
   "source": [
    "# BVL KI Workshop: Customer Demand Prediction\n",
    "## Schritt 1: Lade Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8128c210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation der Libraries im Browser (JupyterLite spezifisch)\n",
    "import piplite\n",
    "await piplite.install(['pandas', 'numpy', 'plotly', 'scikit-learn'])\n",
    "\n",
    "# Standard Libraries für Datenverarbeitung\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Tools für Exploration & Visualisierung\n",
    "# import dtale\n",
    "import plotly.express as px\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a4b35e",
   "metadata": {},
   "source": [
    "## Schritt 2: Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0238e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verbindung zur SQLite Datenbank herstellen\n",
    "con = sqlite3.connect(\"data/data.db\")\n",
    "\n",
    "# Tabellen laden\n",
    "customer = pd.read_sql_query(\"SELECT * FROM customer\", con)\n",
    "plant = pd.read_sql_query(\"SELECT * FROM plant\", con)\n",
    "shipment = pd.read_sql_query(\"SELECT * FROM shipment\", con)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2099173",
   "metadata": {},
   "source": [
    "## Schritt 3: Data Preparation & Exploration (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a92e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Daten zusammenführen (Joins) und Datum konvertieren\n",
    "shpm = (\n",
    "    shipment\n",
    "    .merge(customer, left_on=\"Ship-to2\", right_on=\"ID\", suffixes=(\"\", \"_customer\"))\n",
    "    .merge(plant, left_on=\"Plant\", right_on=\"ID\", suffixes=(\"\", \"_plant\"))\n",
    "    .assign(del_date=lambda x: pd.to_datetime(x[\"Delivery_day\"], format=\"%d.%m.%Y\"))\n",
    ")\n",
    "\n",
    "# 2. Aggregation auf Tagesebene (Zeitreihe erstellen)\n",
    "shpm_ts = (\n",
    "    shpm\n",
    "    .groupby(\"del_date\", as_index=False)\n",
    "    .agg(TO_total=(\"GWkg\", lambda x: np.nansum(x) / 1000)) # Umrechnung in Tonnen\n",
    "    .sort_values(\"del_date\")\n",
    ")\n",
    "\n",
    "# 3. Interaktive Visualisierung\n",
    "fig = px.line(\n",
    "    shpm_ts, x=\"del_date\", y=\"TO_total\",\n",
    "    markers=True,title=f\"Shipments - Calendar Year(s): {shpm_ts['del_date'].dt.year.unique()}\")\n",
    "fig.update_layout(xaxis_title=\"Delivery Date\", yaxis_title=\"Tonnage Delivered\",hovermode=\"x unified\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fd0af9",
   "metadata": {},
   "source": [
    "## Schritt 4: Train the AI model and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4173c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basis-DataFrame für das Modell erstellen\n",
    "df_model = (\n",
    "    shpm\n",
    "    .groupby(\"del_date\", as_index=False)\n",
    "    .agg(TO_total=(\"GWkg\", lambda x: np.nansum(x) / 1000))\n",
    ")\n",
    "\n",
    "# Leere Datenpunkte entfernen und sortieren\n",
    "df_model = df_model.dropna(subset=['del_date']).sort_values('del_date').reset_index(drop=True)\n",
    "\n",
    "# Feature Engineering: Zeit-Merkmale aus dem Datum extrahieren\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    df['dayofweek'] = df['del_date'].dt.dayofweek\n",
    "    df['quarter'] = df['del_date'].dt.quarter\n",
    "    df['month'] = df['del_date'].dt.month\n",
    "    df['year'] = df['del_date'].dt.year\n",
    "    df['dayofyear'] = df['del_date'].dt.dayofyear\n",
    "    df['weekofyear'] = df['del_date'].dt.isocalendar().week.astype(int)\n",
    "    return df\n",
    "\n",
    "df_model = create_features(df_model)\n",
    "\n",
    "# Train/Test Split (80% Training, 20% Test)\n",
    "split_ratio = 0.8\n",
    "split_point = int(len(df_model) * split_ratio)\n",
    "\n",
    "FEATURES = ['dayofweek', 'quarter', 'month', 'year', 'dayofyear', 'weekofyear']\n",
    "TARGET = 'TO_total'\n",
    "\n",
    "# Daten aufteilen\n",
    "train = df_model.iloc[:split_point].copy()\n",
    "test = df_model.iloc[split_point:].copy()\n",
    "\n",
    "X_train = train[FEATURES]\n",
    "y_train = train[TARGET]\n",
    "X_test = test[FEATURES]\n",
    "y_test = test[TARGET]\n",
    "\n",
    "# XGBoost Regressor initialisieren\n",
    "reg = GradientBoostingRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=3,\n",
    "    random_state=42,\n",
    "    validation_fraction=0.1, # Für Early Stopping Simulation\n",
    "    n_iter_no_change=50      # Äquivalent zu early_stopping_rounds\n",
    ")\n",
    "\n",
    "# Modell trainieren\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Vorhersagen für das Test-Set generieren\n",
    "test['Prediction'] = reg.predict(X_test)\n",
    "mae = mean_absolute_error(test[TARGET], test['Prediction'])\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "\n",
    "# Detaillierte Ansicht der Testdaten\n",
    "display(test.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18920959",
   "metadata": {},
   "source": [
    "## Schritt 5: Visualisierung der Ergebnisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31189c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung der Ergebnisse: Training vs. Actual vs. Prediction\n",
    "\n",
    "# 1. Labeling der Trainingsdaten\n",
    "train_plot = train[['del_date', TARGET]].copy()\n",
    "train_plot['Type'] = 'Training Data'\n",
    "train_plot = train_plot.rename(columns={TARGET: 'Tons'})\n",
    "\n",
    "# 2. Labeling der echten Testdaten\n",
    "test_actual_plot = test[['del_date', TARGET]].copy()\n",
    "test_actual_plot['Type'] = 'Test Data (Actual)'\n",
    "test_actual_plot = test_actual_plot.rename(columns={TARGET: 'Tons'})\n",
    "\n",
    "# 3. Labeling der Vorhersagen\n",
    "test_pred_plot = test[['del_date', 'Prediction']].copy()\n",
    "test_pred_plot['Type'] = 'Prediction'\n",
    "test_pred_plot = test_pred_plot.rename(columns={'Prediction': 'Tons'})\n",
    "\n",
    "# 4. Alles in einem DataFrame zusammenführen\n",
    "plot_df = pd.concat([train_plot, test_actual_plot, test_pred_plot], ignore_index=True)\n",
    "\n",
    "# --- GRAPH ERSTELLEN ---\n",
    "fig = px.line(\n",
    "    plot_df, \n",
    "    x='del_date', y='Tons', color='Type', \n",
    "    title=f'Full Timeline: Training, Test & Forecast (MAE: {mae:.2f})',\n",
    "    color_discrete_map={\n",
    "        \"Training Data\": \"gray\",        # Kontext (dezent)\n",
    "        \"Test Data (Actual)\": \"blue\",   # Wahrheit\n",
    "        \"Prediction\": \"red\"             # Prognose\n",
    "    },\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "\n",
    "# Vertikale Linie zur Trennung von Train und Test hinzufügen\n",
    "fig.add_vline(x=test['del_date'].min(), line_width=1, line_dash=\"dash\", line_color=\"black\")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Date\", yaxis_title=\"Volume (Tons)\",\n",
    "    hovermode=\"x unified\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
